{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "metrics_gan.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2895832a009c494fb19ccc80406a148a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_8bce6a27579943e1bd1ec1d96709f305",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_081d26ef01f94fbabbfd776e116ff371",
              "IPY_MODEL_478411d859cf4502881ad92cf78f81c7"
            ]
          }
        },
        "8bce6a27579943e1bd1ec1d96709f305": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "081d26ef01f94fbabbfd776e116ff371": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_15766ae4a3144054b94422064bcd346b",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 87306240,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 87306240,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_88ac5ac4fcea4887be61a12b4e138ae6"
          }
        },
        "478411d859cf4502881ad92cf78f81c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_84696af163544bd0acd24cb7b4103e82",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 83.3M/83.3M [00:00&lt;00:00, 181MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_68688a02870a456397c4ddbe622d8871"
          }
        },
        "15766ae4a3144054b94422064bcd346b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "88ac5ac4fcea4887be61a12b4e138ae6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "84696af163544bd0acd24cb7b4103e82": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "68688a02870a456397c4ddbe622d8871": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jalcg85WlZ7L",
        "outputId": "21db0ddf-c38a-404f-e3d3-2e147578610e"
      },
      "source": [
        "!pip install pot"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pot\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f8/22/67658e4b227fc52ce1c9bca522dfb3f0cc29a3536d1c7499feb3b0042a41/POT-0.7.0-cp36-cp36m-manylinux2010_x86_64.whl (428kB)\n",
            "\r\u001b[K     |▊                               | 10kB 23.9MB/s eta 0:00:01\r\u001b[K     |█▌                              | 20kB 14.3MB/s eta 0:00:01\r\u001b[K     |██▎                             | 30kB 13.5MB/s eta 0:00:01\r\u001b[K     |███                             | 40kB 12.8MB/s eta 0:00:01\r\u001b[K     |███▉                            | 51kB 10.3MB/s eta 0:00:01\r\u001b[K     |████▋                           | 61kB 10.5MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 71kB 10.7MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 81kB 11.0MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 92kB 10.6MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 102kB 11.5MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 112kB 11.5MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 122kB 11.5MB/s eta 0:00:01\r\u001b[K     |██████████                      | 133kB 11.5MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 143kB 11.5MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 153kB 11.5MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 163kB 11.5MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 174kB 11.5MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 184kB 11.5MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 194kB 11.5MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 204kB 11.5MB/s eta 0:00:01\r\u001b[K     |████████████████                | 215kB 11.5MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 225kB 11.5MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 235kB 11.5MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 245kB 11.5MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 256kB 11.5MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 266kB 11.5MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 276kB 11.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 286kB 11.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 296kB 11.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 307kB 11.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 317kB 11.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 327kB 11.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 337kB 11.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 348kB 11.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 358kB 11.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 368kB 11.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 378kB 11.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 389kB 11.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 399kB 11.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 409kB 11.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 419kB 11.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 430kB 11.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: cython>=0.23 in /usr/local/lib/python3.6/dist-packages (from pot) (0.29.21)\n",
            "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.6/dist-packages (from pot) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.16 in /usr/local/lib/python3.6/dist-packages (from pot) (1.18.5)\n",
            "Installing collected packages: pot\n",
            "Successfully installed pot-0.7.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n7v9UfVv5vzI",
        "outputId": "66ba189c-406f-4b2f-99de-1d1b5352531f"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nQyk5QFS1kJJ"
      },
      "source": [
        "import math\r\n",
        "import os\r\n",
        "import timeit\r\n",
        "import math\r\n",
        "\r\n",
        "import numpy as np\r\n",
        "import ot\r\n",
        "import torch\r\n",
        "from torch import nn\r\n",
        "import torch.nn.functional as F\r\n",
        "import torchvision.datasets as dset\r\n",
        "import torchvision.transforms as transforms\r\n",
        "import torchvision.utils as vutils\r\n",
        "import torchvision.models as models\r\n",
        "import pdb\r\n",
        "from tqdm import tqdm\r\n",
        "\r\n",
        "from scipy.stats import entropy\r\n",
        "from numpy.linalg import norm\r\n",
        "from scipy import linalg\r\n",
        "\r\n",
        "\r\n",
        "def giveName(iter):  # 7 digit name.\r\n",
        "    ans = str(iter)\r\n",
        "    return ans.zfill(7)\r\n",
        "\r\n",
        "\r\n",
        "def make_dataset(dataset, dataroot, imageSize):\r\n",
        "    \"\"\"\r\n",
        "    :param dataset: must be in 'cifar10 | lsun | imagenet | folder | lfw | fake'\r\n",
        "    :return: pytorch dataset for DataLoader to utilize\r\n",
        "    \"\"\"\r\n",
        "    if dataset in ['imagenet', 'folder', 'lfw']:\r\n",
        "        # folder dataset\r\n",
        "        dataset = dset.ImageFolder(root=dataroot,\r\n",
        "                                   transform=transforms.Compose([\r\n",
        "                                       transforms.Resize(imageSize),\r\n",
        "                                       transforms.CenterCrop(imageSize),\r\n",
        "                                       transforms.ToTensor(),\r\n",
        "                                       transforms.Normalize(\r\n",
        "                                           (0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\r\n",
        "                                   ]))\r\n",
        "    elif dataset == 'lsun':\r\n",
        "        dataset = dset.LSUN(db_path=dataroot, classes=['bedroom_train'],\r\n",
        "                            transform=transforms.Compose([\r\n",
        "                                transforms.Resize(imageSize),\r\n",
        "                                transforms.CenterCrop(imageSize),\r\n",
        "                                transforms.ToTensor(),\r\n",
        "                                transforms.Normalize(\r\n",
        "                                    (0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\r\n",
        "                            ]))\r\n",
        "    elif dataset == 'cifar10':\r\n",
        "        dataset = dset.CIFAR10(root=dataroot, download=True,\r\n",
        "                               transform=transforms.Compose([\r\n",
        "                                   transforms.Resize(imageSize),\r\n",
        "                                   transforms.ToTensor(),\r\n",
        "                                   transforms.Normalize(\r\n",
        "                                       (0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\r\n",
        "                               ]))\r\n",
        "    elif dataset == 'celeba':\r\n",
        "        dataset = dset.ImageFolder(root=dataroot,\r\n",
        "                                   transform=transforms.Compose([\r\n",
        "                                       transforms.CenterCrop(138),\r\n",
        "                                       transforms.Resize(imageSize),\r\n",
        "                                       transforms.ToTensor(),\r\n",
        "                                       transforms.Normalize(\r\n",
        "                                           (0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\r\n",
        "                                   ]))\r\n",
        "    else:\r\n",
        "        raise Exception('--dataset must be in cifar10 | lsun | imagenet | folder | lfw | fake')\r\n",
        "    assert dataset\r\n",
        "    return dataset\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "def sampleTrue(dataset, imageSize, dataroot, sampleSize, batchSize, saveFolder, workers=4):\r\n",
        "    print('sampling real images ...')\r\n",
        "    saveFolder = saveFolder + '0/'\r\n",
        "\r\n",
        "    dataset = make_dataset(dataset, dataroot, imageSize)\r\n",
        "    dataloader = torch.utils.data.DataLoader(\r\n",
        "        dataset, shuffle=True, batch_size=batchSize, num_workers=int(workers))\r\n",
        "\r\n",
        "    if not os.path.exists(saveFolder):\r\n",
        "        try:\r\n",
        "            os.makedirs(saveFolder)\r\n",
        "        except OSError:\r\n",
        "            pass\r\n",
        "\r\n",
        "    iter = 0\r\n",
        "    for i, data in enumerate(dataloader, 0):\r\n",
        "        img, _ = data\r\n",
        "        for j in range(0, len(img)):\r\n",
        "\r\n",
        "            vutils.save_image(img[j].mul(0.5).add(\r\n",
        "                0.5), saveFolder + giveName(iter) + \".png\")\r\n",
        "            iter += 1\r\n",
        "            if iter >= sampleSize:\r\n",
        "                break\r\n",
        "        if iter >= sampleSize:\r\n",
        "            break\r\n",
        "\r\n",
        "\r\n",
        "class ConvNetFeatureSaver(object):\r\n",
        "    def __init__(self, model='resnet34', workers=4, batchSize=64):\r\n",
        "        '''\r\n",
        "        model: inception_v3, vgg13, vgg16, vgg19, resnet18, resnet34,\r\n",
        "               resnet50, resnet101, or resnet152\r\n",
        "        '''\r\n",
        "        self.model = model\r\n",
        "        self.batch_size = batchSize\r\n",
        "        self.workers = workers\r\n",
        "        if self.model.find('vgg') >= 0:\r\n",
        "            self.vgg = getattr(models, model)(pretrained=True).cuda().eval()\r\n",
        "            self.trans = transforms.Compose([\r\n",
        "                transforms.Resize(224),\r\n",
        "                transforms.ToTensor(),\r\n",
        "                transforms.Normalize((0.485, 0.456, 0.406),\r\n",
        "                                     (0.229, 0.224, 0.225)),\r\n",
        "            ])\r\n",
        "        elif self.model.find('resnet') >= 0:\r\n",
        "            resnet = getattr(models, model)(pretrained=True)\r\n",
        "            resnet.cuda().eval()\r\n",
        "            resnet_feature = nn.Sequential(resnet.conv1, resnet.bn1,\r\n",
        "                                           resnet.relu,\r\n",
        "                                           resnet.maxpool, resnet.layer1,\r\n",
        "                                           resnet.layer2, resnet.layer3,\r\n",
        "                                           resnet.layer4).cuda().eval()\r\n",
        "            self.resnet = resnet\r\n",
        "            self.resnet_feature = resnet_feature\r\n",
        "            self.trans = transforms.Compose([\r\n",
        "                transforms.Resize((224,224)),\r\n",
        "                transforms.ToTensor(),\r\n",
        "                transforms.Normalize((0.485, 0.456, 0.406),\r\n",
        "                                     (0.229, 0.224, 0.225)),\r\n",
        "            ])\r\n",
        "        elif self.model == 'inception' or self.model == 'inception_v3':\r\n",
        "            inception = models.inception_v3(\r\n",
        "                pretrained=True, transform_input=False).cuda().eval()\r\n",
        "            inception_feature = nn.Sequential(inception.Conv2d_1a_3x3,\r\n",
        "                                              inception.Conv2d_2a_3x3,\r\n",
        "                                              inception.Conv2d_2b_3x3,\r\n",
        "                                              nn.MaxPool2d(3, 2),\r\n",
        "                                              inception.Conv2d_3b_1x1,\r\n",
        "                                              inception.Conv2d_4a_3x3,\r\n",
        "                                              nn.MaxPool2d(3, 2),\r\n",
        "                                              inception.Mixed_5b,\r\n",
        "                                              inception.Mixed_5c,\r\n",
        "                                              inception.Mixed_5d,\r\n",
        "                                              inception.Mixed_6a,\r\n",
        "                                              inception.Mixed_6b,\r\n",
        "                                              inception.Mixed_6c,\r\n",
        "                                              inception.Mixed_6d,\r\n",
        "                                              inception.Mixed_7a,\r\n",
        "                                              inception.Mixed_7b,\r\n",
        "                                              inception.Mixed_7c,\r\n",
        "                                              ).cuda().eval()\r\n",
        "            self.inception = inception\r\n",
        "            self.inception_feature = inception_feature\r\n",
        "            self.trans = transforms.Compose([\r\n",
        "                transforms.Resize(299),\r\n",
        "                transforms.ToTensor(),\r\n",
        "                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\r\n",
        "            ])\r\n",
        "        else:\r\n",
        "            raise NotImplementedError\r\n",
        "\r\n",
        "    def save(self, imgFolder, save2disk=False):\r\n",
        "        dataset = dset.ImageFolder(root=imgFolder, transform=self.trans)\r\n",
        "        dataloader = torch.utils.data.DataLoader(\r\n",
        "            dataset, batch_size=self.batch_size, num_workers=self.workers)\r\n",
        "        print('extracting features...')\r\n",
        "        feature_pixl, feature_conv, feature_smax, feature_logit = [], [], [], []\r\n",
        "        for img, _ in tqdm(dataloader):\r\n",
        "            with torch.no_grad():\r\n",
        "                input = img.cuda()\r\n",
        "                if self.model == 'vgg' or self.model == 'vgg16':\r\n",
        "                    fconv = self.vgg.features(input).view(input.size(0), -1)\r\n",
        "                    flogit = self.vgg.classifier(fconv)\r\n",
        "                    # flogit = self.vgg.logitifier(fconv)\r\n",
        "                elif self.model.find('resnet') >= 0:\r\n",
        "                    fconv = self.resnet_feature(\r\n",
        "                        input).mean(3).mean(2).squeeze()\r\n",
        "                    flogit = self.resnet.fc(fconv)\r\n",
        "                elif self.model == 'inception' or self.model == 'inception_v3':\r\n",
        "                    fconv = self.inception_feature(\r\n",
        "                        input).mean(3).mean(2).squeeze()\r\n",
        "                    flogit = self.inception.fc(fconv)\r\n",
        "                else:\r\n",
        "                    raise NotImplementedError\r\n",
        "                fsmax = F.softmax(flogit)\r\n",
        "                feature_pixl.append(img)\r\n",
        "                feature_conv.append(fconv.data.cpu())\r\n",
        "                feature_logit.append(flogit.data.cpu())\r\n",
        "                feature_smax.append(fsmax.data.cpu())\r\n",
        "\r\n",
        "        feature_pixl = torch.cat(feature_pixl, 0).to('cpu')\r\n",
        "        feature_conv = torch.cat(feature_conv, 0).to('cpu')\r\n",
        "        feature_logit = torch.cat(feature_logit, 0).to('cpu')\r\n",
        "        feature_smax = torch.cat(feature_smax, 0).to('cpu')\r\n",
        "\r\n",
        "        if save2disk:\r\n",
        "            torch.save(feature_conv, os.path.join(\r\n",
        "                imgFolder, 'feature_pixl.pth'))\r\n",
        "            torch.save(feature_conv, os.path.join(\r\n",
        "                imgFolder, 'feature_conv.pth'))\r\n",
        "            torch.save(feature_logit, os.path.join(\r\n",
        "                imgFolder, 'feature_logit.pth'))\r\n",
        "            torch.save(feature_smax, os.path.join(\r\n",
        "                imgFolder, 'feature_smax.pth'))\r\n",
        "\r\n",
        "        return feature_pixl, feature_conv, feature_logit, feature_smax\r\n",
        "\r\n",
        "\r\n",
        "def distance(X, Y, sqrt):\r\n",
        "    nX = X.size(0)\r\n",
        "    nY = Y.size(0)\r\n",
        "    X = X.view(nX,-1)\r\n",
        "    X2 = (X*X).sum(1).resize_(nX,1)\r\n",
        "    Y = Y.view(nY,-1)\r\n",
        "    Y2 = (Y*Y).sum(1).resize_(nY,1)\r\n",
        "\r\n",
        "    M = torch.zeros(nX, nY)\r\n",
        "    M.copy_(X2.expand(nX, nY) + Y2.expand(nY, nX).transpose(0, 1) -\r\n",
        "            2 * torch.mm(X, Y.transpose(0, 1)))\r\n",
        "\r\n",
        "    del X, X2, Y, Y2\r\n",
        "\r\n",
        "    if sqrt:\r\n",
        "        M = ((M + M.abs()) / 2).sqrt()\r\n",
        "\r\n",
        "    return M\r\n",
        "\r\n",
        "\r\n",
        "def wasserstein(M, sqrt):\r\n",
        "    if sqrt:\r\n",
        "        M = M.abs().sqrt()\r\n",
        "    emd = ot.emd2([], [], M.numpy())\r\n",
        "\r\n",
        "    return emd\r\n",
        "\r\n",
        "\r\n",
        "class Score_knn:\r\n",
        "    acc = 0\r\n",
        "    acc_real = 0\r\n",
        "    acc_fake = 0\r\n",
        "    precision = 0\r\n",
        "    recall = 0\r\n",
        "    tp = 0\r\n",
        "    fp = 0\r\n",
        "    fn = 0\r\n",
        "    tn = 0\r\n",
        "\r\n",
        "\r\n",
        "def knn(Mxx, Mxy, Myy, k, sqrt):\r\n",
        "    n0 = Mxx.size(0)\r\n",
        "    n1 = Myy.size(0)\r\n",
        "    label = torch.cat((torch.ones(n0), torch.zeros(n1)))\r\n",
        "    M = torch.cat((torch.cat((Mxx, Mxy), 1), torch.cat(\r\n",
        "        (Mxy.transpose(0, 1), Myy), 1)), 0)\r\n",
        "    if sqrt:\r\n",
        "        M = M.abs().sqrt()\r\n",
        "    INFINITY = float('inf')\r\n",
        "    val, idx = (M + torch.diag(INFINITY * torch.ones(n0 + n1))\r\n",
        "                ).topk(k, 0, False)\r\n",
        "\r\n",
        "    count = torch.zeros(n0 + n1)\r\n",
        "    for i in range(0, k):\r\n",
        "        count = count + label.index_select(0, idx[i])\r\n",
        "    pred = torch.ge(count, (float(k) / 2) * torch.ones(n0 + n1)).float()\r\n",
        "\r\n",
        "    s = Score_knn()\r\n",
        "    s.tp = (pred * label).sum()\r\n",
        "    s.fp = (pred * (1 - label)).sum()\r\n",
        "    s.fn = ((1 - pred) * label).sum()\r\n",
        "    s.tn = ((1 - pred) * (1 - label)).sum()\r\n",
        "    s.precision = s.tp / (s.tp + s.fp + 1e-10)\r\n",
        "    s.recall = s.tp / (s.tp + s.fn + 1e-10)\r\n",
        "    s.acc_real = s.tp / (s.tp + s.fn)\r\n",
        "    s.acc_fake = s.tn / (s.tn + s.fp)\r\n",
        "    s.acc = torch.eq(label, pred).float().mean()\r\n",
        "    s.k = k\r\n",
        "\r\n",
        "    return s\r\n",
        "\r\n",
        "\r\n",
        "def mmd(Mxx, Mxy, Myy, sigma):\r\n",
        "    scale = Mxx.mean()\r\n",
        "    Mxx = torch.exp(-Mxx / (scale * 2 * sigma * sigma))\r\n",
        "    Mxy = torch.exp(-Mxy / (scale * 2 * sigma * sigma))\r\n",
        "    Myy = torch.exp(-Myy / (scale * 2 * sigma * sigma))\r\n",
        "    mmd = math.sqrt(Mxx.mean() + Myy.mean() - 2 * Mxy.mean())\r\n",
        "\r\n",
        "    return mmd\r\n",
        "\r\n",
        "\r\n",
        "def entropy_score(X, Y, epsilons):\r\n",
        "    Mxy = distance(X, Y, False)\r\n",
        "    scores = []\r\n",
        "    for epsilon in epsilons:\r\n",
        "        scores.append(ent(Mxy.t(), epsilon))\r\n",
        "\r\n",
        "    return scores\r\n",
        "\r\n",
        "\r\n",
        "def ent(M, epsilon):\r\n",
        "    n0 = M.size(0)\r\n",
        "    n1 = M.size(1)\r\n",
        "    neighbors = M.lt(epsilon).float()\r\n",
        "    sums = neighbors.sum(0).repeat(n0, 1)\r\n",
        "    sums[sums.eq(0)] = 1\r\n",
        "    neighbors = neighbors.div(sums)\r\n",
        "    probs = neighbors.sum(1) / n1\r\n",
        "    rem = 1 - probs.sum()\r\n",
        "    if rem < 0:\r\n",
        "        rem = 0\r\n",
        "    probs = torch.cat((probs, rem*torch.ones(1)), 0)\r\n",
        "    e = {}\r\n",
        "    e['probs'] = probs\r\n",
        "    probs = probs[probs.gt(0)]\r\n",
        "    e['ent'] = -probs.mul(probs.log()).sum()\r\n",
        "\r\n",
        "    return e\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "eps = 1e-20\r\n",
        "def inception_score(X):\r\n",
        "    kl = X * ((X+eps).log()-(X.mean(0)+eps).log().expand_as(X))\r\n",
        "    score = np.exp(kl.sum(1).mean())\r\n",
        "\r\n",
        "    return score\r\n",
        "\r\n",
        "def mode_score(X, Y):\r\n",
        "    kl1 = X * ((X+eps).log()-(X.mean(0)+eps).log().expand_as(X))\r\n",
        "    kl2 = X.mean(0) * ((X.mean(0)+eps).log()-(Y.mean(0)+eps).log())\r\n",
        "    score = np.exp(kl1.sum(1).mean() - kl2.sum())\r\n",
        "\r\n",
        "    return score\r\n",
        "\r\n",
        "\r\n",
        "def fid(X, Y):\r\n",
        "    m = X.mean(0)\r\n",
        "    m_w = Y.mean(0)\r\n",
        "    X_np = X.numpy()\r\n",
        "    Y_np = Y.numpy()\r\n",
        "\r\n",
        "    C = np.cov(X_np.transpose())\r\n",
        "    C_w = np.cov(Y_np.transpose())\r\n",
        "    C_C_w_sqrt = linalg.sqrtm(C.dot(C_w), True).real\r\n",
        "\r\n",
        "    score = m.dot(m) + m_w.dot(m_w) - 2 * m_w.dot(m) + \\\r\n",
        "        np.trace(C + C_w - 2 * C_C_w_sqrt)\r\n",
        "    \r\n",
        "    return np.sqrt(max(score,0))\r\n",
        "\r\n",
        "\r\n",
        "class Score:\r\n",
        "    emd = 0\r\n",
        "    mmd = 0\r\n",
        "    knn = None\r\n",
        "\r\n",
        "\r\n",
        "def compute_score(real, fake, k=1, sigma=1, sqrt=True):\r\n",
        "\r\n",
        "    Mxx = distance(real, real, False)\r\n",
        "    Mxy = distance(real, fake, False)\r\n",
        "    Myy = distance(fake, fake, False)\r\n",
        "\r\n",
        "    s = Score()\r\n",
        "    s.emd = wasserstein(Mxy, sqrt)\r\n",
        "    s.mmd = mmd(Mxx, Mxy, Myy, sigma)\r\n",
        "    s.knn = knn(Mxx, Mxy, Myy, k, sqrt)\r\n",
        "\r\n",
        "    return s\r\n",
        "\r\n",
        "\r\n",
        "def compute_score_raw(dataset_real, imageSize, real_root,fake_root, sampleSize, batchSize,\r\n",
        "                      saveFolder_r, saveFolder_f,  dataset_fake,\r\n",
        "                      conv_model='resnet34', workers=4):\r\n",
        "\r\n",
        "    sampleTrue(dataset_real, imageSize, real_root, sampleSize, batchSize,\r\n",
        "               saveFolder_r, workers=workers)\r\n",
        "    sampleTrue(dataset_fake, imageSize, fake_root, sampleSize, batchSize,\r\n",
        "               saveFolder_r, workers=workers)\r\n",
        "\r\n",
        "    convnet_feature_saver = ConvNetFeatureSaver(model=conv_model,\r\n",
        "                                                batchSize=batchSize, workers=workers)\r\n",
        "    feature_r = convnet_feature_saver.save(saveFolder_r)\r\n",
        "    feature_f = convnet_feature_saver.save(saveFolder_f)\r\n",
        "\r\n",
        "    # 4 feature spaces and 7 scores + incep + modescore + fid\r\n",
        "    score = np.zeros(4 * 7 + 3)\r\n",
        "    for i in range(0, 4):\r\n",
        "        print('compute score in space: ' + str(i))\r\n",
        "        Mxx = distance(feature_r[i], feature_r[i], False)\r\n",
        "        Mxy = distance(feature_r[i], feature_f[i], False)\r\n",
        "        Myy = distance(feature_f[i], feature_f[i], False)\r\n",
        "\r\n",
        "        score[i * 7] = wasserstein(Mxy, True)\r\n",
        "        score[i * 7 + 1] = mmd(Mxx, Mxy, Myy, 1)\r\n",
        "\r\n",
        "        tmp = knn(Mxx, Mxy, Myy, 1, False)\r\n",
        "        score[(i * 7 + 2):(i * 7 + 7)] = \\\r\n",
        "            tmp.acc, tmp.acc_real, tmp.acc_fake, tmp.precision, tmp.recall\r\n",
        "\r\n",
        "    score[28] = inception_score(feature_f[3])\r\n",
        "    score[29] = mode_score(feature_r[3], feature_f[3])\r\n",
        "    score[30] = fid(feature_r[3], feature_f[3])\r\n",
        "    return score\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TukPD1rN4f65"
      },
      "source": [
        "real_root='/content/drive/MyDrive/DL/project/dataset/dongho/evaluate/real'\r\n",
        "fake_root='/content/drive/MyDrive/DL/project/dataset/dongho/evaluate/fake'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295,
          "referenced_widgets": [
            "2895832a009c494fb19ccc80406a148a",
            "8bce6a27579943e1bd1ec1d96709f305",
            "081d26ef01f94fbabbfd776e116ff371",
            "478411d859cf4502881ad92cf78f81c7",
            "15766ae4a3144054b94422064bcd346b",
            "88ac5ac4fcea4887be61a12b4e138ae6",
            "84696af163544bd0acd24cb7b4103e82",
            "68688a02870a456397c4ddbe622d8871"
          ]
        },
        "id": "j00wtCx72byd",
        "outputId": "c2478070-52ab-411a-d9e3-f29c31212c07"
      },
      "source": [
        "score=compute_score_raw(dataset_real='folder', imageSize=(224,224),real_root=real_root,fake_root=fake_root, sampleSize=40, batchSize=2,\r\n",
        "                      saveFolder_r=real_root, saveFolder_f=fake_root,  dataset_fake='folder',\r\n",
        "                      conv_model='resnet34', workers=4)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sampling real images ...\n",
            "sampling real images ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet34-333f7ec4.pth\" to /root/.cache/torch/hub/checkpoints/resnet34-333f7ec4.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2895832a009c494fb19ccc80406a148a",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=87306240.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/50 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "extracting features...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:213: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "100%|██████████| 50/50 [00:04<00:00, 10.07it/s]\n",
            "  0%|          | 0/20 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "extracting features...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 20/20 [00:00<00:00, 36.30it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "compute score in space: 0\n",
            "compute score in space: 1\n",
            "compute score in space: 2\n",
            "compute score in space: 3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_YhS3OsC8qV1",
        "outputId": "8bb28493-16fa-421e-f5e3-53e72b823c0a"
      },
      "source": [
        "print(score[28],score[29],score[30])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "7.76370906829834 0.7807126641273499 0.5490060448646545\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NRhORz4lEs_h"
      },
      "source": [
        "fid_socre=score[30]\r\n",
        "mmd_score=score[22]\r\n",
        "wd_score=score[21]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ulgXHWIC5F1g",
        "outputId": "ebe999c2-6dc1-4319-d9a6-757e428173a3"
      },
      "source": [
        "print('FID:',fid_socre)\r\n",
        "print('MMD:',mmd_score)\r\n",
        "print('WD:',wd_score)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "FID: 0.45682594180107117\n",
            "MMD: 0.2656373413809235\n",
            "WD: 0.4384483402594925\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}